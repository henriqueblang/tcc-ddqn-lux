{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment football failed: No module named 'gfootball'\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import make\n",
    "from lux.game import Game\n",
    "from lux.game_map import Cell, RESOURCE_TYPES, Position\n",
    "from lux.game_objects import Unit\n",
    "from lux.constants import Constants\n",
    "from lux.game_constants import GAME_CONSTANTS\n",
    "from lux import annotate\n",
    "import math, sys\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output \n",
    "from lux.game import Game\n",
    "from lux.game_map import Cell, RESOURCE_TYPES\n",
    "from lux.constants import Constants\n",
    "from lux.game_constants import GAME_CONSTANTS\n",
    "from lux import annotate\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import tensorflow_hub as hub\n",
    "from collections import deque\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(game_state):\n",
    "    # Teh shape of the map\n",
    "    w,h = game_state.map.width, game_state.map.height\n",
    "    # The map of ressources\n",
    "    M = [ [0  if game_state.map.map[j][i].resource==None else game_state.map.map[j][i].resource.amount for i in range(w)]  for j in range(h)]\n",
    "    \n",
    "    M = np.array(M).reshape((h,w,1))\n",
    "    \n",
    "    # The map of units features\n",
    "    U_player = [ [[0,0,0,0,0] for i in range(w)]  for j in range(h)]    \n",
    "    units = game_state.players[0].units\n",
    "    for i in units:\n",
    "        U_player[i.pos.y][i.pos.x] = [i.type,i.cooldown,i.cargo.wood,i.cargo.coal,i.cargo.uranium]\n",
    "    U_player = np.array(U_player)\n",
    "    \n",
    "    U_opponent = [ [[0,0,0,0,0] for i in range(w)]  for j in range(h)]\n",
    "    units = game_state.players[1].units\n",
    "    for i in units:\n",
    "        U_opponent[i.pos.y][i.pos.x] = [i.type,i.cooldown,i.cargo.wood,i.cargo.coal,i.cargo.uranium]\n",
    "\n",
    "    U_opponent = np.array(U_opponent)\n",
    "    \n",
    "    # The map of cities featrues\n",
    "    e = game_state.players[0].cities\n",
    "    C_player = [ [[0,0,0] for i in range(w)]  for j in range(h)]\n",
    "    for k in e:\n",
    "        citytiles = e[k].citytiles\n",
    "        for i in citytiles:\n",
    "            C_player[i.pos.y][i.pos.x] = [i.cooldown,e[k].fuel,e[k].light_upkeep]\n",
    "    C_player = np.array(C_player)\n",
    "\n",
    "    e = game_state.players[1].cities\n",
    "    C_opponent = [ [[0,0,0] for i in range(w)]  for j in range(h)]\n",
    "    for k in e:\n",
    "        citytiles = e[k].citytiles\n",
    "        for i in citytiles:\n",
    "            C_opponent[i.pos.y][i.pos.x] = [i.cooldown,e[k].fuel,e[k].light_upkeep]\n",
    "    C_opponent = np.array(C_opponent)\n",
    "    \n",
    "    # stacking all in one array\n",
    "    E = np.dstack([M,U_opponent,U_player,C_opponent,C_player])\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(s):\n",
    "    inputs = keras.Input(shape=(s,s,17),name = 'The game map')\n",
    "    f = layers.Flatten()(inputs)   \n",
    "    h,w= s,s\n",
    "    f = layers.Dense(w*h,activation = \"sigmoid\")(f)\n",
    "    f = layers.Reshape((h,w,-1))(f)\n",
    "    units = layers.Dense(6,activation = \"softmax\",name = \"Units_actions\")(f)\n",
    "    cities = layers.Dense(2,activation = \"sigmoid\",name = \"Cities_actions\")(f)\n",
    "    output = layers.Concatenate()([units,cities])\n",
    "    model = keras.Model(inputs = inputs, outputs = output)\n",
    "    #model.compile(loss='mse', optimizer=\"adam\")\n",
    "    model.compile(loss='mse', optimizer=\"nadam\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "The game map (InputLayer)       [(None, 12, 12, 17)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 2448)         0           The game map[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 144)          352656      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 12, 12, 1)    0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Units_actions (Dense)           (None, 12, 12, 6)    12          reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Cities_actions (Dense)          (None, 12, 12, 2)    4           reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 12, 12, 8)    0           Units_actions[0][0]              \n",
      "                                                                 Cities_actions[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 352,672\n",
      "Trainable params: 352,672\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model =get_model(12)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direction(action):\n",
    "    return \"csnwe\"[action] if action < 5 else None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_unit_action_valid(unit, action):\n",
    "    height, width = game_state.map.width, game_state.map.height\n",
    "    \n",
    "    to_x = unit.pos.x\n",
    "    to_y = unit.pos.y\n",
    "    \n",
    "    if not unit.can_act():\n",
    "        return False\n",
    "    \n",
    "    # if action == move:\n",
    "    if action < 5:\n",
    "        direction = get_direction(action)\n",
    "\n",
    "        if direction == \"e\":\n",
    "            to_x += 1\n",
    "        elif direction == \"s\":\n",
    "            to_y += 1\n",
    "        elif direction == \"w\":\n",
    "            to_x -= 1\n",
    "        elif direction == \"n\":\n",
    "            to_y -= 1\n",
    "\n",
    "        # Out of bond\n",
    "        if to_x < 0 or to_x >= width or to_y < 0 or to_y >= height:\n",
    "            return False\n",
    "\n",
    "        to_cell = game_state.map.get_cell(to_x, to_y)\n",
    "        to_citytile = to_cell.citytile\n",
    "\n",
    "        # Not citytile and cell already has unit\n",
    "        if to_citytile is None:\n",
    "            has_player_unit = to_cell.has_player_unit(game_state.players[0])\n",
    "            has_opponent_unit = to_cell.has_player_unit(game_state.players[1])\n",
    "            \n",
    "            if has_player_unit or has_opponent_unit:\n",
    "                return False\n",
    "        # Opponent citytile\n",
    "        elif to_citytile.team != 0:\n",
    "            return False\n",
    "    #elif action == build_city:\n",
    "    elif action == 5:\n",
    "        if not unit.can_build(game_state.map):\n",
    "            return False\n",
    "    else: return False\n",
    "    '''elif action == pillage:\n",
    "        to_cell = get_cell(to_x, to_y)\n",
    "\n",
    "        # Not road\n",
    "        if to_cell.road == 0:\n",
    "            return False'''\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_citytile_action_valid(city_tile, action):\n",
    "    if not city_tile.can_act():\n",
    "        return False\n",
    "    \n",
    "    #if action == research:\n",
    "    if action == 6:\n",
    "        pass\n",
    "    #elif action == build_worker or action == build_cart:\n",
    "    elif action == 7:\n",
    "        player = game_state.players[0]\n",
    "        \n",
    "        owned_units = len(player.units)\n",
    "        owned_city_tiles = 0\n",
    "        \n",
    "        for city in player.cities.values():\n",
    "            owned_city_tiles += len(city.citytiles)\n",
    "\n",
    "        if owned_units >= owned_city_tiles:\n",
    "            return False\n",
    "    else: return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_unit_valid_action(unit, options, i=1):\n",
    "    if i == len(options):\n",
    "        return -1\n",
    "    \n",
    "    option = np.argsort(options)[-i]\n",
    "    \n",
    "    if is_unit_action_valid(unit, option):\n",
    "        return option\n",
    "    \n",
    "    return get_best_unit_valid_action(unit, options, i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_city_tile_valid_action(city_tile, options, i=1):\n",
    "    if i == len(options):\n",
    "        return -1\n",
    "    \n",
    "    option = np.argsort(options)[-i]\n",
    "    \n",
    "    if is_citytile_action_valid(city_tile, option):\n",
    "        return option\n",
    "    \n",
    "    return get_best_city_tile_valid_action(city_tile, options, i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_actions(y, player):\n",
    "    actions = []\n",
    "    best_options = np.zeros((game_state.map.width, game_state.map.height), dtype=int)\n",
    "\n",
    "    for unit in player.units:\n",
    "        unit_y, unit_x = unit.pos.y, unit.pos.x\n",
    "\n",
    "        options = y[unit_y][unit_x]\n",
    "        \n",
    "        best_option = get_best_unit_valid_action(unit, options)\n",
    "        best_options[unit_y, unit_x] = best_option\n",
    "\n",
    "        if -1 < best_option < 5:\n",
    "            actions.append(unit.move(get_direction(best_option)))\n",
    "        elif best_option == 5:\n",
    "            actions.append(unit.build_city())\n",
    "            \n",
    "    for city in player.cities.values():\n",
    "        for city_tile in city.citytiles:\n",
    "            city_tile_y, city_tile_x = city_tile.pos.y, city_tile.pos.x\n",
    "            \n",
    "            options = y[city_tile_y][city_tile_x]\n",
    "            \n",
    "            best_option = get_best_city_tile_valid_action(city_tile, options)\n",
    "            best_options[city_tile_y, city_tile_x] = best_option\n",
    "        \n",
    "            if best_option == 6:\n",
    "                actions.append(city_tile.research())\n",
    "            elif best_option == 7:\n",
    "                actions.append(city_tile.build_worker())\n",
    "    \n",
    "    return actions, best_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def get_prediction_actions(y,player):\\n    # move\\n    option = np.argmax(y, axis=2) \\n    # c s n w e build_city & research & buid_worker  \\n    actions = []\\n    for i in player.units:\\n#         print(option.shape,i.pos.y,i.pos.x)\\n        d = \"csnwe#############\"[option[i.pos.y,i.pos.x]]\\n        if option[i.pos.y,i.pos.x]<5:actions.append(i.move(d))\\n        elif option[i.pos.y,i.pos.x]==5 and i.can_build(game_state.map):actions.append(i.build_city())\\n    \\n    city_tiles: List[CityTile] = []\\n    for city in player.cities.values():\\n        for city_tile in city.citytiles:\\n#             city_tiles.append(city_tile)\\n            if option[city_tile.pos.y,city_tile.pos.x]==6:\\n                action = city_tile.research()\\n                actions.append(action)\\n            if option[city_tile.pos.y,city_tile.pos.x]==7:\\n                action = city_tile.build_worker()\\n                actions.append(action)\\n    return actions,option'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def get_prediction_actions(y,player):\n",
    "    # move\n",
    "    option = np.argmax(y, axis=2) \n",
    "    # c s n w e build_city & research & buid_worker  \n",
    "    actions = []\n",
    "    for i in player.units:\n",
    "#         print(option.shape,i.pos.y,i.pos.x)\n",
    "        d = \"csnwe#############\"[option[i.pos.y,i.pos.x]]\n",
    "        if option[i.pos.y,i.pos.x]<5:actions.append(i.move(d))\n",
    "        elif option[i.pos.y,i.pos.x]==5 and i.can_build(game_state.map):actions.append(i.build_city())\n",
    "    \n",
    "    city_tiles: List[CityTile] = []\n",
    "    for city in player.cities.values():\n",
    "        for city_tile in city.citytiles:\n",
    "#             city_tiles.append(city_tile)\n",
    "            if option[city_tile.pos.y,city_tile.pos.x]==6:\n",
    "                action = city_tile.research()\n",
    "                actions.append(action)\n",
    "            if option[city_tile.pos.y,city_tile.pos.x]==7:\n",
    "                action = city_tile.build_worker()\n",
    "                actions.append(action)\n",
    "    return actions,option'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Last_State = {}\n",
    "learning_rate = 0.01\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 0.995\n",
    "game_state = None\n",
    "model = None\n",
    "last_reward = 0\n",
    "W = 0\n",
    "def agent(observation, configuration):\n",
    "    global game_state,epsilon,model,last_reward,W\n",
    "    \n",
    "    ### Do not edit ###\n",
    "    if observation[\"step\"] == 0:\n",
    "        game_state = Game()\n",
    "        game_state._initialize(observation[\"updates\"])\n",
    "        game_state._update(observation[\"updates\"][2:])\n",
    "        game_state.id = observation.player\n",
    "    else:\n",
    "        game_state._update(observation[\"updates\"])\n",
    "    \n",
    "\n",
    "    ### AI Code goes down here! ### \n",
    "    player = game_state.players[observation.player]\n",
    "    opponent = game_state.players[(observation.player + 1) % 2]\n",
    "    width, height = game_state.map.width, game_state.map.height\n",
    "\n",
    "    # Get Prediction of actions\n",
    "    x = get_inputs(game_state)\n",
    "    y = model.predict(np.asarray([x]))[0]\n",
    "    \n",
    "    if random.random()<epsilon:\n",
    "        y = np.random.rand(*y.shape)\n",
    "    print(\"eps \",epsilon,end= \" | \") \n",
    "    actions,option = get_prediction_actions(y,player)\n",
    "    \n",
    "    print(\"Reward\",observation[\"reward\"])\n",
    "\n",
    "    \n",
    "    if observation.player in Last_State:\n",
    "        _x,_y,_player,_option = Last_State[observation.player]\n",
    "        state,next_state,reward = _x,x,observation[\"reward\"]\n",
    "        \n",
    "        # Reward \n",
    "        if reward > last_reward:r=1\n",
    "        elif reward < last_reward:r = -1\n",
    "        else:r = 0\n",
    "        \n",
    "        # Q-learning update\n",
    " \n",
    "        for i in _player.units:\n",
    "            Q1 = _y[i.pos.y,i.pos.x][_option[i.pos.y,i.pos.x]]\n",
    "            Q2 = y[i.pos.y,i.pos.x][_option[i.pos.y,i.pos.x]]\n",
    "            v = r + gamma*(Q2 - Q1)\n",
    "            _y[i.pos.y,i.pos.x][_option[i.pos.y,i.pos.x]] += learning_rate*v\n",
    "\n",
    "        _y = y + learning_rate*_y\n",
    "        \n",
    "        states = [state]\n",
    "        _y_ = [_y]\n",
    "        \n",
    "        model.fit(np.asarray(states),np.asarray(_y_), epochs=1, verbose=1)\n",
    "        if epsilon > epsilon_final:\n",
    "            epsilon*= epsilon_decay\n",
    "    Last_State[observation.player] = [x,y,player,option]\n",
    "    last_reward = observation[\"reward\"]\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Episode 9 ===\n",
      "eps  0.2 | Reward 0\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0013\n",
      "eps  0.199 | Reward 10001\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.2529e-05\n",
      "eps  0.19800500000000001 | Reward 10001\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.5337e-05\n",
      "eps  0.197014975 | Reward 10001\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1698\n",
      "eps  0.19602990012500002 | Reward 10001\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.3936e-05\n",
      "eps  0.19504975062437502 | Reward 10001\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.0174e-05\n",
      "eps  0.19407450187125314 | Reward 10001\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2026e-05\n",
      "eps  0.19310412936189686 | Reward 10001\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.7528e-06\n",
      "eps  0.19213860871508737 | Reward 10001\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0014e-05\n",
      "eps  0.19117791567151193 | Reward 10001\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.8005e-06\n",
      "eps  0.19022202609315436 | Reward 10001\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.8006e-06\n",
      "eps  0.18927091596268858 | Reward 10001\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1639\n",
      "eps  0.18832456138287512 | Reward 10001\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.2895e-05\n",
      "eps  0.18738293857596075 | Reward 10001\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.8003e-06\n",
      "eps  0.18644602388308096 | Reward 10001\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1682\n",
      "eps  0.18551379376366556 | Reward 20001\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.4341e-05\n",
      "eps  0.18458622479484724 | Reward 20001\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.8284e-06\n",
      "eps  0.183663293670873 | Reward 20001\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.7900e-06\n",
      "eps  0.18274497720251864 | Reward 20001\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.6777e-05\n",
      "eps  0.18183125231650604 | Reward 20001\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.8252e-06\n",
      "eps  0.18092209605492351 | Reward 20001\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.8236e-06\n",
      "eps  0.1800174855746489 | Reward 20001\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.8332e-06\n",
      "eps  0.17911739814677563 | Reward 20001\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.8602e-06\n",
      "eps  0.17822181115604174 | Reward 20001\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.8228e-06\n",
      "eps  0.17733070210026153 | Reward 20001\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.9550e-05\n",
      "eps  0.17644404858976023 | Reward 20001\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1826\n",
      "eps  0.17556182834681144 | Reward 20001\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.5139e-05\n",
      "eps  0.1746840192050774 | Reward 20001\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1679\n",
      "eps  0.173810599109052 | Reward 20001\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.3235e-05\n",
      "eps  0.17294154611350673 | Reward 20001\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1665\n",
      "eps  0.1720768383829392 | Reward 20001\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.4269e-05\n",
      "eps  0.1712164541910245 | Reward 1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.8287e-06\n",
      "eps  0.17036037192006936 | Reward 1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1685\n",
      "eps  0.16950857006046902 | Reward 1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.2941e-05\n",
      "eps  0.16866102721016668 | Reward 1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.8183e-06\n",
      "eps  0.16781772207411585 | Reward 1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.8136e-06\n",
      "eps  0.16697863346374528 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.8095e-06\n",
      "eps  0.16614374029642656 | Reward 1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.8063e-06\n",
      "eps  0.16531302159494443 | Reward 1\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.8036e-06\n",
      "eps  0.16448645648696972 | Reward 1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.8015e-06\n",
      "eps  0.16366402420453488 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1631\n",
      "eps  0.1628457040835122 | Reward 1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1697\n",
      "eps  0.16203147556309463 | Reward 1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.2651e-05\n",
      "eps  0.16122131818527916 | Reward 1\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 9.7884e-06\n",
      "eps  0.16041521159435276 | Reward 1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.7825e-06\n",
      "eps  0.159613135536381 | Reward 1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.7776e-06\n",
      "eps  0.15881506985869911 | Reward 1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.7733e-06\n",
      "eps  0.15802099450940563 | Reward 1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.7699e-06\n",
      "eps  0.1572308895368586 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.7672e-06\n",
      "eps  0.1564447350891743 | Reward 1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.7645e-06\n",
      "eps  0.15566251141372844 | Reward 1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.7628e-06\n",
      "eps  0.1548841988566598 | Reward 1\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.7710e-06\n",
      "eps  0.1541097778623765 | Reward 1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.7671e-06\n",
      "eps  0.15333922897306462 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1756\n",
      "eps  0.1525725328281993 | Reward 1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.4709e-05\n",
      "eps  0.1518096701640583 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.7779e-06\n",
      "eps  0.151050621813238 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.7763e-06\n",
      "eps  0.15029536870417182 | Reward 1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1765\n",
      "eps  0.14954389186065095 | Reward 1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.5228e-05\n",
      "eps  0.1487961724013477 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.7721e-06\n",
      "eps  0.14805219153934096 | Reward 1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1056e-05\n",
      "eps  0.14731193058164427 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.7739e-06\n",
      "eps  0.14657537092873604 | Reward 1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.7897e-06\n",
      "eps  0.14584249407409236 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.7893e-06\n",
      "eps  0.1451132816037219 | Reward 1\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 9.7893e-06\n",
      "eps  0.1443877151957033 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1684\n",
      "eps  0.14366577661972477 | Reward 1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.4088e-05\n",
      "eps  0.14294744773662615 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1698\n",
      "eps  0.14223271049794303 | Reward 1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.2884e-05\n",
      "eps  0.14152154694545332 | Reward 1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.7405e-06\n",
      "eps  0.14081393921072605 | Reward 1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.7359e-06\n",
      "eps  0.1401098695146724 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.7321e-06\n",
      "eps  0.13940932016709906 | Reward 1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1659\n",
      "eps  0.13871227356626356 | Reward 1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.2751e-05\n",
      "eps  0.13801871219843223 | Reward 1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.7217e-06\n",
      "eps  0.13732861863744006 | Reward 1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.7176e-06\n",
      "eps  0.13664197554425286 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.7143e-06\n",
      "eps  0.1359587656665316 | Reward 1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1651\n",
      "eps  0.13527897183819892 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps  0.13460257697900793 | Reward 1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1785\n",
      "eps  0.13392956409411289 | Reward 1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.4013e-05\n",
      "eps  0.1332599162736423 | Reward 1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.6897e-06\n",
      "eps  0.1325936166922741 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.6824e-06\n",
      "eps  0.13193064860881273 | Reward 1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.6765e-06\n",
      "eps  0.13127099536576867 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0774e-05\n",
      "eps  0.13061464038893983 | Reward 1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.6695e-06\n",
      "eps  0.12996156718699514 | Reward 1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.6810e-06\n",
      "eps  0.12931175935106015 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.6777e-06\n",
      "eps  0.12866520055430486 | Reward 1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.6751e-06\n",
      "eps  0.12802187455153333 | Reward 1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.6730e-06\n",
      "eps  0.12738176517877567 | Reward 1\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.6714e-06\n",
      "eps  0.1267448563528818 | Reward 1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.6702e-06\n",
      "eps  0.12611113207111738 | Reward 1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 9.6694e-06\n",
      "eps  0.1254805764107618 | Reward 1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.6690e-06\n",
      "eps  0.12485317352870799 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1695\n",
      "eps  0.12422890766106445 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.3144e-05\n",
      "eps  0.12360776312275913 | Reward 1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.6668e-06\n",
      "eps  0.12298972430714533 | Reward 1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.6647e-06\n",
      "eps  0.1223747756856096 | Reward 1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.6640e-06\n",
      "eps  0.12176290180718155 | Reward 1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.6660e-06\n",
      "eps  0.12115408729814564 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4078e-05\n",
      "eps  0.12054831686165492 | Reward 1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.6762e-06\n",
      "eps  0.11994557527734664 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.6807e-06\n",
      "eps  0.11934584740095991 | Reward 1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.6812e-06\n",
      "eps  0.11874911816395511 | Reward 1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.6819e-06\n",
      "eps  0.11815537257313534 | Reward 1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.6828e-06\n",
      "eps  0.11756459571026966 | Reward 1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.6890e-06\n",
      "eps  0.1169767727317183 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1655\n",
      "eps  0.1163918888680597 | Reward 1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1761\n",
      "eps  0.11580992942371941 | Reward 1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1748\n",
      "eps  0.11523087977660082 | Reward 1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.2901e-05\n",
      "eps  0.11465472537771781 | Reward 1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.6690e-06\n",
      "eps  0.11408145175082922 | Reward 1\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1748\n",
      "eps  0.11351104449207507 | Reward 1\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.4372e-05\n",
      "eps  0.11294348926961469 | Reward 1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.6350e-06\n",
      "eps  0.11237877182326661 | Reward 1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.6285e-06\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "\n",
    "# RL training\n",
    "#sizes = [12,16,24,32]\n",
    "sizes = [12]\n",
    "\n",
    "for size in sizes:\n",
    "    # Inistialise the model\n",
    "    model= get_model(size)\n",
    "    Last_State = {}\n",
    "    for eps in range(episodes):\n",
    "        epsilon = 0.2 # Maintaining exploration\n",
    "        clear_output()\n",
    "        print(\"=== Episode {} ===\".format(eps))\n",
    "        env = make(\"lux_ai_2021\", debug=True, configuration={\"annotations\": True, \"width\":size, \"height\":size})\n",
    "        steps = env.run([\"simple_agent\", agent])\n",
    "    # Save the model\n",
    "    model.save_weights(\"model_%d.h5\"%size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
